{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a86df364",
   "metadata": {},
   "source": [
    "Blog: https://blog.futuresmart.ai/fine-tuning-hugging-face-transformers-model        \n",
    "YouTube: https://www.youtube.com/watch?v=9he4XKqqzvE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd392d9",
   "metadata": {},
   "source": [
    "We hereby fine-tune the DistilBERT model for the sentiment analysis task using the following Toxic Comment dataset.\n",
    "\n",
    "Dataset Link: https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e717dd54",
   "metadata": {},
   "source": [
    "- Install the transformers library using `!pip install transformers -U` and import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c2fec84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers -U\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f6cd67",
   "metadata": {},
   "source": [
    "- Load the dataset containing toxic comments using `pd.read_csv` and select only the relevant columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dae86c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6n/twfkd47s7c1_g13g57szr4d80000gn/T/ipykernel_19148/3533485564.py:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(\"toxic-comments.csv\",error_bad_lines=False, engine=\"python\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"toxic-comments.csv\",error_bad_lines=False, engine=\"python\")\n",
    "data = data[['comment_text','toxic']]\n",
    "data = data[0:1000]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf4c340",
   "metadata": {},
   "source": [
    "- Split the dataset into training and validation sets using `train_test_split` from sklearn. 80% of the data is used for training and 20% for validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd03b8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(data[\"comment_text\"])\n",
    "y = list(data[\"toxic\"])\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2,stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95a3c71",
   "metadata": {},
   "source": [
    "- Tokenize the input sequences using the `DistilBertTokenizer` from the transformers library. The `padding=True`, `truncation=True`, and `max_length=512` parameters ensure that all input sequences have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3a05da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)\n",
    "X_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=512)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5890fe8",
   "metadata": {},
   "source": [
    "- Create a custom PyTorch dataset using the tokenized input sequences and corresponding labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "324e2f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "train_dataset = Dataset(X_train_tokenized, y_train)\n",
    "val_dataset = Dataset(X_val_tokenized, y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413f0a21",
   "metadata": {},
   "source": [
    "In this snippet, we define a custom dataset class `Dataset` that takes the tokenized encodings and labels as input. The `__getitem__` method returns a dictionary of tensors representing each input sample. We create instances of `train_dataset` and `val_dataset` using the tokenized datasets and labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9fbf86",
   "metadata": {},
   "source": [
    "- Define a function to compute evaluation metrics (accuracy, recall, precision, and F1-score) for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "418dffe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred)\n",
    "    precision = precision_score(y_true=labels, y_pred=pred)\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred)\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fdd46f",
   "metadata": {},
   "source": [
    "- Define the training arguments for the Trainer using `TrainingArguments`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bcf7d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"distilbert-base-uncased\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f765dd0",
   "metadata": {},
   "source": [
    "- Define the Trainer using the `Trainer` class from the transformers library, passing in the model, training, and validation datasets, and the evaluation metrics function. Then, we train the model using `trainer.train()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "383e82d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/titus/opt/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 13:07, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=0.24767805099487306, metrics={'train_runtime': 797.0641, 'train_samples_per_second': 1.004, 'train_steps_per_second': 0.125, 'total_flos': 105973918924800.0, 'train_loss': 0.24767805099487306, 'epoch': 1.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "model = model.to(device)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533316c5",
   "metadata": {},
   "source": [
    "- Evaluate the model and make predictions on new inputs: Here, we define the training arguments and initialize the `Trainer` with the DistilBERT model, training arguments, and the training and validation datasets. We train the model using `trainer.train()` and evaluate its performance on the validation set using `trainer.evaluate()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53605cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0.978746  , 0.02125401]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()\n",
    "\n",
    "text = \"That was good point\"\n",
    "inputs = tokenizer(text, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "outputs = model(**inputs)\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "predictions = predictions.cpu().detach().numpy()\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1508b04",
   "metadata": {},
   "source": [
    "The output has a 0.97 score for a positive label indicating that the text was a positive comment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a98a3f5",
   "metadata": {},
   "source": [
    "We save the fine-tuned model using `trainer.save_model()` to a directory called `bert-base-uncased-finetuned-toxic-comments`. Later, we load the saved model using `DistilBertForSequenceClassification.from_pretrained()` and move it to the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56567681",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('distilbert-base-uncased-finetuned-toxic-comments')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05865dd",
   "metadata": {},
   "source": [
    "We can load the saved model and make predictions on new inputs.\n",
    "\n",
    "Finally, we provide an example text and tokenize it. We pass the tokenized inputs through the loaded model (`model_2`) to obtain the predicted probabilities using the `softmax` function. The predictions are converted to a numpy array for further processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44c127aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.19095181, 0.8090482 ]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-toxic-comments\")\n",
    "model_2.to(device)\n",
    "\n",
    "text = \"go to hell\"\n",
    "inputs = tokenizer(text, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "outputs = model_2(**inputs)\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "predictions = predictions.cpu().detach().numpy()\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332b56b9",
   "metadata": {},
   "source": [
    "Here we have a 0.80 score for the negative label, indicating that the text represents a negative comment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e512c336",
   "metadata": {},
   "source": [
    "This code demonstrates how to fine-tune a pre-trained DistilBERT model on a custom dataset using the Hugging Face Transformers library. The model is trained to classify toxic comments, and evaluation metrics such as accuracy, recall, precision, and F1-score are computed to assess the model's performance. Finally, the trained model is saved and can be loaded later to make predictions on new inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90cd19e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
